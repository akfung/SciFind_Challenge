{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/amosfung/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script runs through all the authors on a pubmed paper using its url. \n",
    "It collects the abstracts of the 5 most recent papers of each author credited in that paper.\n",
    "\"\"\"\n",
    "\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "\n",
    "# Chromedriver setup\n",
    "def init_browser():\n",
    "    executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "    return Browser('chrome', **executable_path, headless=False) #set to false for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting abstracts for Cheryl Keech\n",
      "Collecting abstracts for Gary Albert\n",
      "Collecting abstracts for Iksung Cho\n",
      "Collecting abstracts for Andreana Robertson\n",
      "Collecting abstracts for Patricia Reed\n",
      "Collecting abstracts for Susan Neal\n",
      "Collecting abstracts for Joyce S Plested\n",
      "Collecting abstracts for Mingzhu Zhu\n",
      "Collecting abstracts for Shane Cloney-Clark\n",
      "Collecting abstracts for Haixia Zhou\n",
      "Collecting abstracts for Gale Smith\n",
      "Collecting abstracts for Nita Patel\n",
      "Collecting abstracts for Matthew B Frieman\n",
      "Collecting abstracts for Robert E Haupt\n",
      "Collecting abstracts for James Logue\n",
      "Collecting abstracts for Marisa McGrath\n",
      "Collecting abstracts for Stuart Weston\n",
      "Collecting abstracts for Pedro A Piedra\n",
      "Collecting abstracts for Chinar Desai\n",
      "Collecting abstracts for Kathleen Callahan\n",
      "Collecting abstracts for Maggie Lewis\n",
      "Collecting abstracts for Patricia Price-Abbott\n",
      "Collecting abstracts for Neil Formica\n",
      "Collecting abstracts for Vivek Shinde\n",
      "Collecting abstracts for Louis Fries\n",
      "Collecting abstracts for Jason D Lickliter\n",
      "Collecting abstracts for Paul Griffin\n",
      "Collecting abstracts for Bethanie Wilkinson\n",
      "Collecting abstracts for Gregory M Glenn\n"
     ]
    }
   ],
   "source": [
    "#visit the web page\n",
    "browser = init_browser()\n",
    "\n",
    "base_url = \"https://pubmed.ncbi.nlm.nih.gov\" #pubmed base url\n",
    "article_url = \"https://pubmed.ncbi.nlm.nih.gov/32877576/\"\n",
    "\n",
    "#get list of authors and hrefs to their searches\n",
    "browser.visit(article_url)\n",
    "soup = bs(browser.html, \"html.parser\")\n",
    "authors = []\n",
    "for child in soup.find(\"div\", class_=\"authors\").find_all(\"a\", class_='full-name'):\n",
    "    author_name = child.get_text()\n",
    "    author_href = child['href']\n",
    "    authors.append({\n",
    "        \"name\": author_name,\n",
    "        \"href\": author_href\n",
    "    })\n",
    "\n",
    "for author in authors:\n",
    "    print(f'Collecting abstracts for {author[\"name\"]}')\n",
    "          \n",
    "    #go to date sorted results for author\n",
    "    browser.visit(base_url + author['href'] + \"&sort=date\")\n",
    "    soup = bs(browser.html, \"html.parser\")\n",
    "          \n",
    "    article_hrefs = [] # list to hold hrefs for 5 most recent articles by scientist\n",
    "    for article in soup.find_all(\"a\", class_=\"docsum-title\", limit=5):\n",
    "        article_hrefs.append(article['href'])\n",
    "    abstract_string = ''\n",
    "          \n",
    "    #go to each article by href and add the abstract text to a string if it exists\n",
    "    for article_href in article_hrefs:\n",
    "        browser.visit(base_url + article_href)\n",
    "        soup = bs(browser.html, \"html.parser\")\n",
    "        abstract = soup.find(id=\"enc-abstract\")\n",
    "        if abstract:\n",
    "            abstract_string += abstract.get_text()\n",
    "          \n",
    "    #add the concatenated string to authors dict\n",
    "    author[\"abstracts\"] = abstract_string\n",
    "browser.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df with data and cleaning\n",
    "df = pd.DataFrame(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanHtml(sentence): #regex to remove end of sentence punctuation\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', str(sentence))\n",
    "    return cleantext\n",
    "\n",
    "\n",
    "def cleanPunc(sentence): #function to clean the abstracts of any punctuation or special characters\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    cleaned = cleaned.strip()\n",
    "    cleaned = cleaned.replace(\"\\n\",\" \")\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# stem data to combine words with similar meanings w/ snowball stemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def stemming(sentence):\n",
    "    stemSentence = \"\"\n",
    "    for word in sentence.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stemSentence += stem\n",
    "        stemSentence += \" \"\n",
    "    stemSentence = stemSentence.strip()\n",
    "    return stemSentence\n",
    "\n",
    "#update list of stopwords from nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#remove some common terms used in pubmed abstract\n",
    "stop_words.update(['background', 'methods', 'results', 'conclusions'])\n",
    "re_stop_words = re.compile(r\"\\b(\" + \"|\".join(stop_words) + \")\\\\W\", re.I)\n",
    "#function to remove stop words\n",
    "def removeStopWords(sentence):\n",
    "    global re_stop_words\n",
    "    return re_stop_words.sub(\" \", sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply cleaning functions to abstract text\n",
    "df['abstracts'] = df['abstracts'].str.lower()\n",
    "df['abstracts'] = df['abstracts'].apply(cleanHtml)\n",
    "df['abstracts'] = df['abstracts'].apply(cleanPunc)\n",
    "#optionally use stemming\n",
    "# df['abstracts'] = df['abstracts'].apply(stemming)\n",
    "df['abstracts'] = df['abstracts'].apply(removeStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"authors.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PythonData] *",
   "language": "python",
   "name": "conda-env-PythonData-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
